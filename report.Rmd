---
title: "Reproducible Research: Peer Assessment 2"
output: 
  html_document:
    keep_md: true
---

```{r global, , echo=FALSE}
opts_chunk$set(echo = TRUE, cache = TRUE)
options(scipen = 1)
```

# Reproducible Research Assignment 2
========================================================

### by *Thomas Antonakis* on `r date()`

## Effects of Storm events on population health and economic damage


## Synopsis

Immediately after the title, there should be a synopsis which describes and summarizes your analysis in at most 10 complete sentences.

## Data Processing

There should be a section titled Data Processing which describes (in words and code) how the data were loaded into R and processed for analysis. In particular, your analysis must start from the raw CSV file containing the data. You cannot do any preprocessing outside the document. If preprocessing is time-consuming you may consider using the cache = TRUE option for certain code chunks.

This project involves exploring the U.S. National Oceanic and Atmospheric Administration's (NOAA) storm database. THis database tracks characteristics of major storms and weather events in the United States, including when and where they occur, as well as estimates of any fatalities, injuries, property and crop damage. 

The events in the database start in the year 1950 and end in November 2011. In the earlier years of the database there are generally fewer events recorded, most likely due to lack of good records. More recent years should be considered more complete. 

The data for the  analysis came in form of a comma-separated-value file compressed via the bzip2 algorithm to reduce its size.  
Let's first of all download the file.  

``` {r download the bz2 file, cache=TRUE}
# Create folder to put download the file
if(!file.exists("./data")){dir.create("./data")}

# Download the file, and keep the date. 
if(!file.exists("./data/storm_data.csv.bz2")){
fileurl<-"https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2"
download.file(fileurl, destfile="./data/storm_data.csv.bz2", method="auto")
dateDownloaded<-date()
}
```  

The file is now downloaded to a local folder. We now will unzip it and load it into a dataset in the R environment.  

*Loading the whole dataset in R, caused knitr to stop its executionand not showing anything from this point down in the report. The loading was done in R studio , some exploration was performed and below we will only load / read the variables that seem to be related with the questions of the assignment.*  

```{r unzip and load storm data}
# "Unzip file" to a variable
filename <- bzfile("./data/storm_data.csv.bz2")

# Load file in a dataframe. 
# We do not need all variables. After checking the documentation we keep the following
if (!exists("storm")){
storm<-read.csv(filename, stringsAsFactors = FALSE, 
                colClasses=c("NULL", NA   ,"NULL","NULL","NULL",
                             "NULL","NULL", NA   ,"NULL","NULL",
                             "NULL","NULL","NULL","NULL","NULL",
                             "NULL","NULL","NULL","NULL","NULL",
                             "NULL","NULL",NA,NA,NA,
                             NA,NA,NA,"NULL","NULL",
                             "NULL","NULL","NULL","NULL","NULL",
                             "NULL","NULL"))
}
```  

This may take a couple of minutes depending on the sustem, as the csv is 47Mb big compressed and uncompressed is much much more than that: 548Mb.  
So, hopefully, after the csv has been loaded in a dataframe named `storm` , we can check it out a little bit.

```{r basic exploration of the storm file, cache=FALSE}
# Check the file out
dim(storm)
str(storm)
summary(storm)
```

The initial dataframe contains `r dim(storm)[1]` observations and `r dim(storm)[2]` variables which are explained below:  

1. BGN_DATE is for now a character variable which contains the date of the record.  
2. EVTYPE is a character variable which contains the type of event.  
3. FATALITIES is a numeric vector containing the number of deaths caused by a specific observation.  
4. INJURIESis a numeric vector containing the number of injured persons caused by a specific observation.  
5. PROPDMG is a numeric vector containing an estimate from the researcher of the economic damage caused by a specific observation in units (see next variable) on prroperties.  
6. PROPDMGEXP is a character variable which should contain a letter, ideally one of the following "H","K","M","B". These letters should stand for Hundreds, Thousands, Millions, and Billions.  
7. CROPDMG is a numeric vector containing an estimate from the researcher of the economic damage caused by a specific observation in units (see next variable) on crops.
8. CROPDMGEXP is a character variable which should contain a letter, ideally one of the following "H","K","M","B". These letters should stand for Hundreds, Thousands, Millions, and Billions.  

We will transform the BGN_DATE variable to the year of the record. Knowing that the records span from 1950 to 2011 which is a very long period of time, days, months do not really matter.

```{r transform into year}
# Fix dates Convert date and time to YEAR
storm$BGN_DATE <- as.numeric(format(as.Date(storm$BGN_DATE, format = "%m/%d/%Y %H:%M:%S"), "%Y"))
```

We will calculate the quantiles of the distribution of the years below.
```{r quantiles}
quantile(storm$BGN_DATE, c(seq(from=0.1, to=1, by = 0.1)))
```

Let's make a histogram of the number of observations per year.
```{r init hist for years}
# Histogram the years for the count of records 
hist(storm$BGN_DATE, main = "Number of storm events per year", 
     xlab = "Year")
```

In the earlier years of the database there are generally fewer events recorded, most likely due to lack of good records. More recent years should be considered more complete.  We will take out of the analysis records who date earlier than 1980.  

The proportion of observations before 1980 are the `r round(sum(storm$BGN_DATE<1980)/nrow(storm)*100, 2)`% of total observations, and will nw be dropped out.  

```{r take out old observations}
# Decide which years to keep
storm<-storm[storm$BGN_DATE>=1980,]
```

Having now made up our minds about the years we will use, we will create an intermediate file that will be used from now on for the anaysis. The old dataframe will be deleted for memory reasons.

```{r intermediate}
# We do not need all variables. After checking the documentation we keep the following
file_intermediate<-data.frame("EVTYPE" = storm$EVTYPE, "FATALITIES" = storm$FATALITIES,
                              "INJURIES" = storm$INJURIES, "PROPDMG" = storm$PROPDMG, 
                              "PROPDMGEXP" = storm$PROPDMGEXP, "CROPDMG" = storm$CROPDMG, 
                              "CROPDMGEXP" = storm$CROPDMGEXP)

# Release memory
rm(storm)
```

Let us take another look at the summary and structure of the intermediate file:

```{r intermediate check}
# Check the intermediate file out
str(file_intermediate)
summary(file_intermediate)
```

## Results

There should be a section titled Results in which your results are presented.

The analysis document must have at least one figure containing a plot.

Your analyis must have no more than three figures. Figures may have multiple plots in them (i.e. panel plots), but there cannot be more than three figures total.

We will first tackle the analysis on health effects of the storms. So, we will check the sums of the injuries and the fatalities of storms recorded after 1980.  

```{r sums of health}
#Check the variables connected to population health effects
sum(file_intermediate$FATALITIES)
sum(file_intermediate$INJURIES)
```

There have been `r sum(file_intermediate$FATALITIES)` deaths and `r sum(file_intermediate$INJURIES)`  injuries from storm events from 1980 onwards.   

Let us take a subset of the intermediate file using only the event types and the variables of injuries and fatalities.

```{r subset for health}
# Store health related variables to a separate dataframe.
health<-data.frame("EVTYPE" = file_intermediate$EVTYPE, 
                   "FATALITIES" = file_intermediate$FATALITIES,
                   "INJURIES" = file_intermediate$INJURIES)

# "Clean" data frame names
names(health)<-tolower(names(health))
```

We need to discuss a bit the comparison of an injury and a fatality on the effect they have on population health. We can all understand that  a minor scratch might be recorded as an injury, as a paralysis can also be cosidered as an injury too, but a fatality means a lot more than that, but it is more specific.  
In order to point that difference out we will multiply the effect of a fatality so that a fatality is considered 10 times more harmful as an innjury record, and we could even be quite underestimating the comparison.  
So an index of health effect will be created in order to combine the fatalities and the injuries data. 

```{r health index} 
# Calculate a health damage index using the fatalities and injuries variables
# We assume that one fatality weighs as much as 10 injuries in terms of health damage
health$damage <- health$injuries + 10 * health$fatalities
```

We now have to aggregate the effect on the event types, so as to find out which types are the more harmful for the population health, across the United States.

We will simply add all the figures in the index to see the total damage caused, and simultaneously, we will try the same things but with averages, so as to calculate an effect per event.

```{r aggregate}
# Calculate sum of damages and average damage per evtype
library(plyr)
health_sum<-ddply(.data=health, .variables=.(evtype) , summarize, sum = sum(damage))
health_ave<-ddply(.data=health, .variables=.(evtype) , summarize, sum = mean(damage))
names(health_sum) [2]<- c("sum_damage")
names(health_ave) [2]<- c("avg_damage")
health_agg<-arrange(join(health_sum, health_ave), evtype)
a<-head(arrange(health_agg, sum_damage, decreasing = TRUE), 10)
a
head(arrange(health_agg, avg_damage, decreasing = TRUE), 10)
```

It turns out that the average version did not work as expected as the mean equals the sum , so these types of storm events have occured too few times.

So, using the total effect we show in the following plot which tyes of storms have had the greates effects on population health.  

```{r barplot health}
# Sum will be used
# Make a plot to illustrate greatest threats
aplot <- a$sum_damage
names(aplot) <- a$evtype
par(mar = c(4, 10, 4, 1))
barplot(aplot, col= 2, main="Top types of events in \n total health Damage index", 
        horiz=TRUE, las=1)

# Release memory
rm(health_sum,health_ave)
```

The most harmful in terms of population health storm types are in decreasing order: `r head(a[,1])`


We will now tackle the analysis on the economic damage caused by storms. In out intermediate file there are four variables that are related to economic damage.

5. PROPDMG is a numeric vector containing an estimate from the researcher of the economic damage caused by a specific observation in units (see next variable) on prroperties.  
6. PROPDMGEXP is a character variable which should contain a letter, ideally one of the following "H","K","M","B". These letters should stand for Hundreds, Thousands, Millions, and Billions.  
7. CROPDMG is a numeric vector containing an estimate from the researcher of the economic damage caused by a specific observation in units (see next variable) on crops.
8. CROPDMGEXP is a character variable which should contain a letter, ideally one of the following "H","K","M","B". These letters should stand for Hundreds, Thousands, Millions, and Billions. 

So, actually there are two sectors that we can calculate economic damage: 

* Property damage
* Crops damage

The way we will calculate the damage in each sector is the following:  
We will multiply the actual number written down by the researcher (in variable `****DMG`) with a factor that will be created through the `****DMGEXP` variables. 

```{r economic explore the multipliers}
# Explore the variables connected to monetary damages.
table(file_intermediate$PROPDMGEXP, useNA="ifany")
table(file_intermediate$CROPDMGEXP, useNA="ifany")

# Upper-case everything
file_intermediate$PROPDMGEXP<-toupper(file_intermediate$PROPDMGEXP)
file_intermediate$CROPDMGEXP<-toupper(file_intermediate$CROPDMGEXP)

# Assign everyhing that is not empty or "H", K", "M", "B" to Missing value
file_intermediate$PROPDMGEXP[!file_intermediate$PROPDMGEXP %in% 
                                     c("", "H", "K","M","B")]<-NA
file_intermediate$CROPDMGEXP[!file_intermediate$CROPDMGEXP %in% 
                                     c("", "H", "K","M","B")]<-NA
```

So, after a first cleanup, let us take a look at the new `****DMGEXP`s
```{r new dmgexps}
# Summarize new exponents
table(file_intermediate$PROPDMGEXP, useNA="ifany")
table(file_intermediate$CROPDMGEXP, useNA="ifany")
```

We bring the new variables to a new clean dataset to be used for the economic analysis:

```{r economic}
# Store money related variables to a separate dataframe.
economic<-data.frame("EVTYPE" = file_intermediate$EVTYPE, 
                   "PROPDMG" = file_intermediate$PROPDMG, 
                   "PROPDMGEXP" = file_intermediate$PROPDMGEXP, 
                   "CROPDMG" = file_intermediate$CROPDMG, 
                   "CROPDMGEXP" = file_intermediate$CROPDMGEXP)

# "Clean" data frame names
names(economic)<-tolower(names(economic))

# Release memory
rm(file_intermediate)
```

As described in the above list, apart from other characters that we will deal with later, the `****DMGEXP` variables contain mostly the letters `H,T,M,B` , which stand for Hundreds, Thousands, Millions, and Billions.  So, a transformation need to be done so as to have the actual multiplier. 

Please note that we assigned anything that did not look like "H", "K","M","B" or an empty record to a missing values, and we plan to take those observations out. Before we do that we will calculate how many are to be taken out as a share of the total observations (after 1980)

```{r calculate missing share }
# Calculate share of missing values
good<- complete.cases(economic$propdmgexp,economic$cropdmgexp)
round(abs(sum(good)/nrow(economic)-1)*100, 2)
```

The share of missing values is only `r round(abs(sum(good)/nrow(economic)-1)*100, 2)`% of the total, so we will indeed take them out.

```{r take out NAs}
# Take Missing values out of economic dataframe
economic<-economic[good,]

# Release memory
rm(good)
```

Lets
