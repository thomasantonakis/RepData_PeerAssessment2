---
title: "Reproducible Research: Peer Assessment 2"
output: 
  html_document:
    keep_md: true
---

```{r global, , echo=FALSE}
opts_chunk$set(echo = TRUE, cache = TRUE)
```

# Reproducible Research Assignment 2
========================================================

## *Thomas Antonakis*

### Title

Your document should have a title that briefly summarizes your data analysis


### Synopsis

Immediately after the title, there should be a synopsis which describes and summarizes your analysis in at most 10 complete sentences.

### Data Processing

There should be a section titled Data Processing which describes (in words and code) how the data were loaded into R and processed for analysis. In particular, your analysis must start from the raw CSV file containing the data. You cannot do any preprocessing outside the document. If preprocessing is time-consuming you may consider using the cache = TRUE option for certain code chunks.

This project involves exploring the U.S. National Oceanic and Atmospheric Administration's (NOAA) storm database. THis database tracks characteristics of major storms and weather events in the United States, including when and where they occur, as well as estimates of any fatalities, injuries, property and crop damage.  
The events in the database start in the year 1950 and end in November 2011. In the earlier years of the database there are generally fewer events recorded, most likely due to lack of good records. More recent years should be considered more complete.  
The data for the  analysis came in form of a comma-separated-value file compressed via the bzip2 algorithm to reduce its size.  
Let's first of all download the file.  
``` {r download the bz2 file, cache=TRUE}
# Create folder to put download the file
if(!file.exists("./data")){dir.create("./data")}

# Download the file, and keep the date. 
if(!file.exists("./data/storm_data.csv.bz2")){
fileurl<-"https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2"
download.file(fileurl, destfile="./data/storm_data.csv.bz2", method="auto")
dateDownloaded<-date()
}
```  

The file is now downloaded to a local folder. We now will unzip it and load it into a dataset in the R environment.  

*Loading the whole dataset in R, caused knitr to stop its executionand not showing anything from this point down in the report. The loading was done in R studio , some exploration was performed and below we will only load / read the variables that seem to be related with the questions of the assignment.*  

```{r unzip and load storm data}
# "Unzip file" to a variable
filename <- bzfile("./data/storm_data.csv.bz2")

# Load file in a dataframe. 
# We do not need all variables. After checking the documentation we keep the following
if (!exists("storm")){
storm<-read.csv(filename, stringsAsFactors = FALSE, 
                colClasses=c("NULL", NA   ,"NULL","NULL","NULL",
                             "NULL","NULL", NA   ,"NULL","NULL",
                             "NULL","NULL","NULL","NULL","NULL",
                             "NULL","NULL","NULL","NULL","NULL",
                             "NULL","NULL",NA,NA,NA,
                             NA,NA,NA,"NULL","NULL",
                             "NULL","NULL","NULL","NULL","NULL",
                             "NULL","NULL"))
}
```  

This may take a couple of minutes depending on the sustem, as the csv is 47Mb big compressed and uncompressed is much much more than that: 548Mb.  
So, hopefully, after the csv has been loaded in a dataframe named `storm` , we can check it out a little bit.

```{r basic exploration of the storm file, cache=FALSE}
# Check the file out
dim(storm)
str(storm)
summary(storm)
```

The initial dataframe contains `r dim(storm)[1]` observations and `r dim(storm)[2]` variables which are explained below:  
1. BGN_DATE is for now a character variable which contains the date of the record.  
2. EVTYPE is a character variable which contains the type of event.  
3. FATALITIES is a numeric vector containing the number of deaths caused by a specific observation.  
4. INJURIESis a numeric vector containing the number of injured persons caused by a specific observation.  
5. PROPDMG is a numeric vector containing an estimate from the researcher of the economic damage caused by a specific observation in units (see next variable) on prroperties.  
6. PROPDMGEXP is a character variable which should contain a letter, ideally one of the following "H","K","M","B". These letters should stand for Hundreds, Thousands, Millions, and Billions.  
7. CROPDMG is a numeric vector containing an estimate from the researcher of the economic damage caused by a specific observation in units (see next variable) on crops.
8. CROPDMGEXPis a character variable which should contain a letter, ideally one of the following "H","K","M","B". These letters should stand for Hundreds, Thousands, Millions, and Billions.  

We will transform the BGN_DATE variable to the year of the record. Knowing that the records span from 1950 to 2011, days, months do not really matter.

```{r transform into year}
# Fix dates Convert date and time to YEAR
storm$BGN_DATE <- as.numeric(format(as.Date(storm$BGN_DATE, format = "%m/%d/%Y %H:%M:%S"), "%Y"))
```

We will calculate the quantiles of the distribution of the years below.
```{r quantiles}
quantile(storm$BGN_DATE, c(seq(from=0.1, to=1, by = 0.1)))
```

Let's make a histogram of the number of observations per year.
```{r init hist for years}
# Histogram the years for the count of records 
hist(storm$BGN_DATE, main = "Number of storm events per year", 
     xlab = "Year")
```

In the earlier years of the database there are generally fewer events recorded, most likely due to lack of good records. More recent years should be considered more complete.  We will take out of the analysis records who date earlier than 1980.  

The propostion of observations before 1980 are the `r round(sum(storm$BGN_DATE<1980)/nrow(storm)*100, 2)`% of total observations, and will nw be dropped out.  

```{r take out old observations}
# Decide which years to keep
storm<-storm[storm$BGN_DATE>=1980,]
```

Having now made up our minds about the years we will use, we will create an intermediate file that will be used from now on for the anaysis. The old dataframe will be deleted for memory reasons.

```{r intermediate}
# We do not need all variables. After checking the documentation we keep the following
file_intermediate<-data.frame("EVTYPE" = storm$EVTYPE, "FATALITIES" = storm$FATALITIES,
                              "INJURIES" = storm$INJURIES, "PROPDMG" = storm$PROPDMG, 
                              "PROPDMGEXP" = storm$PROPDMGEXP, "CROPDMG" = storm$CROPDMG, 
                              "CROPDMGEXP" = storm$CROPDMGEXP)

# Release memory
rm(storm)
```

Let us take another look at the summary and structure of the intermediate file:

```{r}
# Check the intermediate file out
str(file_intermediate)
summary(file_intermediate)
```

We will first tackle the analysis on health effects of the storms. So, we will check the sums of the injuries and the fatalities of storms recorded after 1980.  

```{r sums of health}
#Check the variables connected to population health effects
sum(file_intermediate$FATALITIES)
sum(file_intermediate$INJURIES)
```

There have been `r sum(file_intermediate$FATALITIES)`deaths and `r sum(file_intermediate$INJURIES)`  injuries from storm events from 1980 onwards.   

Let us take a subset of the intermediate file using only the event types and the variables of injuries and fatalities.

```{r subset for health}
# Store health related variables to a separate dataframe.
health<-data.frame("EVTYPE" = file_intermediate$EVTYPE, 
                   "FATALITIES" = file_intermediate$FATALITIES,
                   "INJURIES" = file_intermediate$INJURIES)

# "Clean" data frame names
names(health)<-tolower(names(health))
```

We need to discuss a bit the comparison of an injury and a fatality on the effect they have on population health. We can all understand that  a minor scratch might be recorded as an injury, as a paralysis can also be cosidered as an injury too, but a fatality means a lot more than that, but it is more specific.  
In order to point that difference out we will multiply the effect of a fatality so that a fatality is considered 10 times more harmful as an innjury record, and we could even be quite underestimating the comparison.  
So an index of health effect will be created in order to combine the fatalities and the injuries data. 

```{r health index} 
# Calculate a health damage index using the fatalities and injuries variables
# We assume that one fatality weighs as much as 10 injuries in terms of health damage
health$damage <- health$injuries + 10 * health$fatalities
```

We now have to aggregate the effect on the event types, so as to find out which types are the more harmful for the population health, across the United States.

We will simply add all the figures in the index to see the total damage caused, and simultaneously, we will try the same things but with averages, so as to calculate an effect per event.

```{r aggregate}
# Calculate sum of damages and average damage per evtype
library(plyr)
health_sum<-ddply(.data=health, .variables=.(evtype) , summarize, sum = sum(damage))
health_ave<-ddply(.data=health, .variables=.(evtype) , summarize, sum = mean(damage))
names(health_sum) [2]<- c("sum_damage")
names(health_ave) [2]<- c("avg_damage")
health_agg<-arrange(join(health_sum, health_ave), evtype)
a<-head(arrange(health_agg, sum_damage, decreasing = TRUE), 10)
a
head(arrange(health_agg, avg_damage, decreasing = TRUE), 10)
```

It turns out that the average version did not work as expected as the mean equals the sum , so these types of storm events have occured too few times.

So, using the total effect we shhow in the following plot which tyes of storms have had the greates effects on population health.  

```{r barplot health}
# Sum will be used
# Make a plot to illustrate greatest threats
aplot <- a$sum_damage
names(aplot) <- a$evtype
par(mar = c(4, 10, 4, 1))
barplot(aplot, col= 2, main="Top types of events in \n total health Damage index", 
        horiz=TRUE, las=1)

# Release memory
rm(health_sum,health_ave)
```

### Results

There should be a section titled Results in which your results are presented.

The analysis document must have at least one figure containing a plot.

Your analyis must have no more than three figures. Figures may have multiple plots in them (i.e. panel plots), but there cannot be more than three figures total.
